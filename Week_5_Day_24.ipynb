{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1e53188d128e416bb5c5879a51f9acfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextareaModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextareaModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextareaView",
            "continuous_update": true,
            "description": "Prompt:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_29ee8e5d4e2d47b4932219a98d069c38",
            "placeholder": "Enter text prompt",
            "rows": null,
            "style": "IPY_MODEL_829753df7aaa4aa8aca8dda62d8da758",
            "value": ""
          }
        },
        "29ee8e5d4e2d47b4932219a98d069c38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": "100px",
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "829753df7aaa4aa8aca8dda62d8da758": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8337ae04ff34dc5832761a40c8f519c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Generate Text",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_44405cac8bd34264a04ccc09daf9b871",
            "style": "IPY_MODEL_3e98277707cd4925855ce7c6226d9189",
            "tooltip": ""
          }
        },
        "44405cac8bd34264a04ccc09daf9b871": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e98277707cd4925855ce7c6226d9189": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "c3aa2b08073b4249bb6aa7d46257a4a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextareaModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextareaModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextareaView",
            "continuous_update": true,
            "description": "Output:",
            "description_tooltip": null,
            "disabled": true,
            "layout": "IPY_MODEL_59f4c254b61b4fd0832da6f694ddc462",
            "placeholder": "Generated text will appear here",
            "rows": null,
            "style": "IPY_MODEL_96ed186ef88244859cce18ad835045a7",
            "value": "Once upon a time, the world was a place of great beauty and great danger. The world of the gods was the place where the great gods were born, and where they were to live.\n\nThe world that was created was not the same as the one that is now. It was an endless, endless world. And the Gods were not born of nothing. They were created of a single, single thing. That was why the universe was so beautiful. Because the cosmos was made of two"
          }
        },
        "59f4c254b61b4fd0832da6f694ddc462": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": "200px",
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "96ed186ef88244859cce18ad835045a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2cbb017722ee4bb8866b07f65b2b331e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextareaModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextareaModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextareaView",
            "continuous_update": true,
            "description": "Input:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_fbb836b7ad674c1f831336881ba677db",
            "placeholder": "Enter text to translate",
            "rows": null,
            "style": "IPY_MODEL_4fcab4f707dc4f8f8523a79a6c806ccb",
            "value": "Hello, how are you?"
          }
        },
        "fbb836b7ad674c1f831336881ba677db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": "100px",
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "4fcab4f707dc4f8f8523a79a6c806ccb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "556003638c05459cb0734f61f58252f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Translate",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_9647425c7eb54507803219a3d4a877d2",
            "style": "IPY_MODEL_7a97ff7c9dd2482cb12b223ddc41b630",
            "tooltip": ""
          }
        },
        "9647425c7eb54507803219a3d4a877d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a97ff7c9dd2482cb12b223ddc41b630": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "b4b73f4e7ae34f59936680c9d377a352": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextareaModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextareaModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextareaView",
            "continuous_update": true,
            "description": "Output:",
            "description_tooltip": null,
            "disabled": true,
            "layout": "IPY_MODEL_8c04952105f14a08bcd4f4941f72d25d",
            "placeholder": "Translated text will appear here",
            "rows": null,
            "style": "IPY_MODEL_473216271af64c93ac3de0eacec3b9ef",
            "value": "Bonjour, comment ça va?"
          }
        },
        "8c04952105f14a08bcd4f4941f72d25d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": "100px",
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "473216271af64c93ac3de0eacec3b9ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **BOOTCAMP @ GIKI (Content designed by Usama Arshad) WEEK 5**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "DPrOS2-AnoXJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Classification (Sentiment Analysis) using BERT\n",
        "\n",
        "## Theory:\n",
        "\n",
        "### Transformers:\n",
        "Transformers are a type of deep learning model introduced by Vaswani et al. in their 2017 paper \"Attention is All You Need.\" Transformers have revolutionized natural language processing (NLP) and many other fields. They are the foundation of many state-of-the-art models, including BERT, GPT, and T5.\n",
        "\n",
        "#### Key Concepts of Transformers:\n",
        "\n",
        "**Attention Mechanism:**\n",
        "- The core idea of transformers is the self-attention mechanism, which allows the model to weigh the importance of different words in a sentence.\n",
        "- Self-attention helps the model focus on relevant parts of the input sequence, improving the ability to understand context and relationships between words.\n",
        "\n",
        "**Encoder-Decoder Architecture:**\n",
        "- Transformers are typically composed of an encoder and a decoder.\n",
        "- The encoder processes the input sequence and generates a context-aware representation.\n",
        "- The decoder uses this representation to generate the output sequence (e.g., translation, summary).\n",
        "\n",
        "**Parallel Processing:**\n",
        "- Unlike RNNs, which process sequences sequentially, transformers process entire sequences in parallel.\n",
        "- This makes transformers much more efficient and faster to train.\n",
        "\n",
        "### BERT (Bidirectional Encoder Representations from Transformers):\n",
        "BERT is a transformer-based model designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. This allows BERT to understand the context of a word from both directions, which improves performance on a wide range of NLP tasks.\n",
        "\n",
        "#### Key Features of BERT:\n",
        "\n",
        "**Bidirectional:**\n",
        "- BERT reads text in both directions (left-to-right and right-to-left) to understand the context of words better.\n",
        "\n",
        "**Pre-training and Fine-tuning:**\n",
        "- BERT is pre-trained on a large corpus of text using two tasks: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP).\n",
        "- After pre-training, BERT can be fine-tuned on specific tasks (e.g., sentiment analysis, question answering) with relatively few additional parameters.\n",
        "\n",
        "**Transformers Encoder:**\n",
        "- BERT uses only the transformer encoder architecture, discarding the decoder since it's not needed for many NLP tasks.\n",
        "\n",
        "### BERT Tokenizer:\n",
        "The BERT tokenizer is responsible for converting raw text into the input format required by BERT. It includes several steps:\n",
        "\n",
        "**Tokenization:**\n",
        "- The text is split into individual tokens (words or subwords).\n",
        "\n",
        "**Adding Special Tokens:**\n",
        "- Special tokens such as [CLS] (start of sequence) and [SEP] (separator) are added to the tokenized text.\n",
        "\n",
        "**Padding and Truncation:**\n",
        "- Sequences are padded to the same length or truncated if they are too long.\n",
        "\n",
        "**Conversion to Input IDs and Attention Masks:**\n",
        "- Tokens are converted into numerical IDs that BERT can process.\n",
        "- Attention masks are created to indicate which tokens should be attended to (1) and which should be ignored (0).\n"
      ],
      "metadata": {
        "id": "wVqJXZDWqHSA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Classification (Sentiment Analysis) using BERT\n",
        "\n",
        "## Theory:\n",
        "BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model that has achieved state-of-the-art performance on many NLP tasks, including text classification. BERT is pre-trained on a large corpus of text and can be fine-tuned for specific tasks such as sentiment analysis. Sentiment analysis is the task of classifying the sentiment of a given text, such as determining whether a movie review is positive or negative.\n",
        "\n",
        "## Steps:\n",
        "\n",
        "### 1. Load and Preprocess the Dataset\n",
        "We'll use the IMDB dataset, which contains 50,000 movie reviews labeled as positive or negative. We'll split the data into training and test sets, and then tokenize and encode the text data.\n",
        "\n",
        "### 2. Fine-tune the BERT Model\n",
        "We'll use the pre-trained BERT model from Hugging Face's `transformers` library and fine-tune it on the IMDB dataset. Fine-tuning involves training the model on the specific task of sentiment analysis for a few epochs.\n",
        "\n",
        "### 3. Make Predictions and Evaluate the Model\n",
        "We'll use the fine-tuned model to make predictions on the test data and evaluate its performance using classification metrics such as accuracy and the classification report.\n",
        "\n",
        "### 4. Save and Load the Model\n",
        "We'll save the fine-tuned model and demonstrate how to load it for inference. We'll also provide a function to predict the sentiment of new text inputs.\n",
        "\n"
      ],
      "metadata": {
        "id": "XYtiMl_aop7y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8Sz7mw8njW1",
        "outputId": "69427195-5eb2-4c69-9985-d3b9e6467205"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            " 259/1407 [====>.........................] - ETA: 7:58:40 - loss: 0.4477 - accuracy: 0.7806"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np  # For numerical computations\n",
        "import tensorflow as tf  # For building and training the BERT model\n",
        "from sklearn.metrics import classification_report  # For evaluating the model's performance\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification  # For BERT tokenizer and model\n",
        "import tensorflow_datasets as tfds  # For loading the IMDB dataset\n",
        "import ipywidgets as widgets  # For creating UI widgets\n",
        "from IPython.display import display  # For displaying UI elements\n",
        "\n",
        "# Load the IMDB dataset\n",
        "data, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)  # Load IMDB dataset with info\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train_data, test_data = data['train'], data['test']  # Split data into training and test sets\n",
        "\n",
        "# Initialize the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')  # Use the pre-trained BERT tokenizer\n",
        "\n",
        "# Function to tokenize and encode the data\n",
        "def encode_data(dataset, max_length=128):\n",
        "    input_ids, attention_masks, labels = [], [], []  # Initialize lists for input IDs, attention masks, and labels\n",
        "\n",
        "    for text, label in tfds.as_numpy(dataset):  # Iterate through the dataset\n",
        "        encoding = tokenizer.encode_plus(\n",
        "            text.decode('utf-8'),  # Decode text from bytes to string\n",
        "            max_length=max_length,  # Maximum length of the tokenized text\n",
        "            truncation=True,  # Truncate texts longer than max length\n",
        "            padding='max_length',  # Pad texts shorter than max length\n",
        "            add_special_tokens=True,  # Add special tokens (e.g., [CLS], [SEP])\n",
        "            return_attention_mask=True,  # Return attention masks\n",
        "            return_tensors='tf'  # Return TensorFlow tensors\n",
        "        )\n",
        "        input_ids.append(encoding['input_ids'])  # Append input IDs\n",
        "        attention_masks.append(encoding['attention_mask'])  # Append attention masks\n",
        "        labels.append(label)  # Append labels\n",
        "\n",
        "    return tf.concat(input_ids, axis=0), tf.concat(attention_masks, axis=0), tf.convert_to_tensor(labels)  # Concatenate and return tensors\n",
        "\n",
        "# Encode the training and test data\n",
        "X_train, X_train_masks, y_train = encode_data(train_data)  # Encode the training data\n",
        "X_test, X_test_masks, y_test = encode_data(test_data)  # Encode the test data\n",
        "\n",
        "# Initialize the BERT model for sequence classification\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  # Load pre-trained BERT model for classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),  # Use Adam optimizer with a learning rate of 2e-5\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  # Use sparse categorical cross-entropy loss\n",
        "              metrics=['accuracy'])  # Track accuracy metric\n",
        "\n",
        "# Train the model\n",
        "history = model.fit([X_train, X_train_masks], y_train, epochs=3, batch_size=16, validation_split=0.1)  # Train the model for 3 epochs\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict([X_test, X_test_masks])  # Predict labels for the test data\n",
        "y_pred_labels = np.argmax(y_pred.logits, axis=1)  # Get predicted labels\n",
        "\n",
        "# Evaluate the model\n",
        "print(classification_report(y_test, y_pred_labels))  # Print classification report\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained('./sentiment-analysis-bert')  # Save the fine-tuned BERT model\n",
        "\n",
        "# Load the model for inference\n",
        "loaded_model = TFBertForSequenceClassification.from_pretrained('./sentiment-analysis-bert')  # Load the saved model\n",
        "\n",
        "# Function to predict sentiment\n",
        "def predict_sentiment(text):\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        text,  # Text to be tokenized\n",
        "        max_length=128,  # Maximum length of the tokenized text\n",
        "        truncation=True,  # Truncate texts longer than max length\n",
        "        padding='max_length',  # Pad texts shorter than max length\n",
        "        add_special_tokens=True,  # Add special tokens (e.g., [CLS], [SEP])\n",
        "        return_attention_mask=True,  # Return attention masks\n",
        "        return_tensors='tf'  # Return TensorFlow tensors\n",
        "    )\n",
        "    input_ids = encoding['input_ids']  # Get input IDs\n",
        "    attention_mask = encoding['attention_mask']  # Get attention masks\n",
        "    logits = loaded_model(input_ids, attention_mask=attention_mask).logits  # Get logits from the model\n",
        "    return np.argmax(logits)  # Return predicted label\n",
        "\n",
        "# Test the prediction function\n",
        "print(predict_sentiment(\"I loved this movie! It was amazing.\"))  # Predict sentiment for a positive review\n",
        "print(predict_sentiment(\"I hated this movie. It was terrible.\"))  # Predict sentiment for a negative review\n",
        "\n",
        "# UI Components\n",
        "\n",
        "# Define widgets for user input and output\n",
        "input_text = widgets.Textarea(\n",
        "    value='I loved this movie! It was amazing.',\n",
        "    placeholder='Enter text to analyze',\n",
        "    description='Input:',\n",
        "    layout=widgets.Layout(width='100%', height='100px')\n",
        ")\n",
        "\n",
        "output_text = widgets.Textarea(\n",
        "    value='',\n",
        "    placeholder='Predicted sentiment will appear here',\n",
        "    description='Output:',\n",
        "    layout=widgets.Layout(width='100%', height='100px'),\n",
        "    disabled=True\n",
        ")\n",
        "\n",
        "analyze_button = widgets.Button(description='Analyze Sentiment')\n",
        "\n",
        "# Define the button click event handler\n",
        "def on_analyze_button_clicked(b):\n",
        "    sentiment = predict_sentiment(input_text.value)\n",
        "    sentiment_text = \"Positive\" if sentiment == 1 else \"Negative\"\n",
        "    output_text.value = sentiment_text\n",
        "\n",
        "# Attach the event handler to the button\n",
        "analyze_button.on_click(on_analyze_button_clicked)\n",
        "\n",
        "# Display the widgets\n",
        "display(input_text, analyze_button, output_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Generation using GPT-2\n",
        "\n",
        "## Theory:\n",
        "\n",
        "### GPT (Generative Pre-trained Transformer):\n",
        "GPT is a transformer-based model developed by OpenAI that is designed for generating text. Unlike BERT, which is designed for understanding text, GPT is designed for generating coherent and contextually relevant text.\n",
        "\n",
        "#### Key Concepts of GPT:\n",
        "\n",
        "**Autoregressive Model:**\n",
        "- GPT is an autoregressive model, which means it generates text one token at a time and each token is conditioned on the previously generated tokens.\n",
        "\n",
        "**Transformer Decoder:**\n",
        "- GPT uses only the transformer decoder architecture, unlike BERT, which uses the encoder. The decoder is designed to generate sequences.\n",
        "\n",
        "**Pre-training and Fine-tuning:**\n",
        "- GPT is pre-trained on a large corpus of text in an unsupervised manner. It is then fine-tuned on specific tasks such as text generation, machine translation, or question answering.\n",
        "\n",
        "### GPT-2:\n",
        "GPT-2 is a variant of GPT that has been trained on a large dataset to generate human-like text. It can generate coherent and contextually relevant text when provided with a prompt.\n",
        "\n",
        "## Implementation:\n",
        "\n",
        "### 1. Load and Prepare the GPT-2 Model\n",
        "We'll use the pre-trained GPT-2 model from Hugging Face's `transformers` library.\n",
        "\n",
        "### 2. Generate Text\n",
        "We'll use the model to generate text based on a given prompt.\n"
      ],
      "metadata": {
        "id": "IpGwYRk-ta_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Initialize the GPT-2 tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Load the pre-trained GPT-2 model\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Function to generate text\n",
        "def generate_text(prompt, max_length=100):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt')  # Encode the prompt text\n",
        "    output = model.generate(input_ids, max_length=max_length, num_return_sequences=1, no_repeat_ngram_size=2, top_k=50, top_p=0.95, temperature=1.9)  # Generate text\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)  # Decode the generated text\n",
        "    return generated_text\n",
        "\n",
        "# Define widgets for user input and output\n",
        "input_prompt = widgets.Textarea(\n",
        "    value='Once upon a time',\n",
        "    placeholder='Enter text prompt',\n",
        "    description='Prompt:',\n",
        "    layout=widgets.Layout(width='100%', height='100px')\n",
        ")\n",
        "\n",
        "output_text = widgets.Textarea(\n",
        "    value='',\n",
        "    placeholder='Generated text will appear here',\n",
        "    description='Output:',\n",
        "    layout=widgets.Layout(width='100%', height='200px'),\n",
        "    disabled=True\n",
        ")\n",
        "\n",
        "generate_button = widgets.Button(description='Generate Text')\n",
        "\n",
        "# Define the button click event handler\n",
        "def on_generate_button_clicked(b):\n",
        "    generated = generate_text(input_prompt.value)\n",
        "    output_text.value = generated\n",
        "\n",
        "# Attach the event handler to the button\n",
        "generate_button.on_click(on_generate_button_clicked)\n",
        "\n",
        "# Display the widgets\n",
        "display(input_prompt, generate_button, output_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412,
          "referenced_widgets": [
            "1e53188d128e416bb5c5879a51f9acfc",
            "29ee8e5d4e2d47b4932219a98d069c38",
            "829753df7aaa4aa8aca8dda62d8da758",
            "f8337ae04ff34dc5832761a40c8f519c",
            "44405cac8bd34264a04ccc09daf9b871",
            "3e98277707cd4925855ce7c6226d9189",
            "c3aa2b08073b4249bb6aa7d46257a4a2",
            "59f4c254b61b4fd0832da6f694ddc462",
            "96ed186ef88244859cce18ad835045a7"
          ]
        },
        "id": "Yvel5favtclh",
        "outputId": "16fbcb5e-a6cb-492d-8259-931414030013"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Textarea(value='Once upon a time', description='Prompt:', layout=Layout(height='100px', width='100%'), placeho…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e53188d128e416bb5c5879a51f9acfc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(description='Generate Text', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f8337ae04ff34dc5832761a40c8f519c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Textarea(value='', description='Output:', disabled=True, layout=Layout(height='200px', width='100%'), placehol…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c3aa2b08073b4249bb6aa7d46257a4a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Translation using MarianMT\n",
        "\n",
        "## Theory:\n",
        "\n",
        "### MarianMT:\n",
        "MarianMT is a sequence-to-sequence model trained specifically for machine translation. It uses the transformer architecture to translate text from one language to another.\n",
        "\n",
        "## Implementation:\n",
        "\n",
        "### 1. Load and Prepare the MarianMT Model\n",
        "We'll use the pre-trained MarianMT model from Hugging Face's `transformers` library.\n",
        "\n",
        "### 2. Translate Text\n",
        "We'll use the model to translate text from English to French.\n"
      ],
      "metadata": {
        "id": "GMBgaInhtiyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Define the model name for English to French translation\n",
        "model_name = 'Helsinki-NLP/opus-mt-en-fr'\n",
        "\n",
        "# Initialize the MarianMT tokenizer\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load the pre-trained MarianMT model\n",
        "model = MarianMTModel.from_pretrained(model_name)\n",
        "\n",
        "# Function to translate text\n",
        "def translate_text(text, max_length=100):\n",
        "    input_ids = tokenizer.encode(text, return_tensors='pt', max_length=max_length, truncation=True)  # Encode the input text\n",
        "    output = model.generate(input_ids, max_length=max_length, num_beams=4, early_stopping=True)  # Generate translation\n",
        "    translated_text = tokenizer.decode(output[0], skip_special_tokens=True)  # Decode the translated text\n",
        "    return translated_text\n",
        "\n",
        "# Define widgets for user input and output\n",
        "input_text = widgets.Textarea(\n",
        "    value='Hello, how are you?',\n",
        "    placeholder='Enter text to translate',\n",
        "    description='Input:',\n",
        "    layout=widgets.Layout(width='100%', height='100px')\n",
        ")\n",
        "\n",
        "output_text = widgets.Textarea(\n",
        "    value='',\n",
        "    placeholder='Translated text will appear here',\n",
        "    description='Output:',\n",
        "    layout=widgets.Layout(width='100%', height='100px'),\n",
        "    disabled=True\n",
        ")\n",
        "\n",
        "translate_button = widgets.Button(description='Translate')\n",
        "\n",
        "# Define the button click event handler\n",
        "def on_translate_button_clicked(b):\n",
        "    translated = translate_text(input_text.value)\n",
        "    output_text.value = translated\n",
        "\n",
        "# Attach the event handler to the button\n",
        "translate_button.on_click(on_translate_button_clicked)\n",
        "\n",
        "# Display the widgets\n",
        "display(input_text, translate_button, output_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277,
          "referenced_widgets": [
            "2cbb017722ee4bb8866b07f65b2b331e",
            "fbb836b7ad674c1f831336881ba677db",
            "4fcab4f707dc4f8f8523a79a6c806ccb",
            "556003638c05459cb0734f61f58252f9",
            "9647425c7eb54507803219a3d4a877d2",
            "7a97ff7c9dd2482cb12b223ddc41b630",
            "b4b73f4e7ae34f59936680c9d377a352",
            "8c04952105f14a08bcd4f4941f72d25d",
            "473216271af64c93ac3de0eacec3b9ef"
          ]
        },
        "id": "aUoAk4fkuxMj",
        "outputId": "3fb45152-7897-435e-fa6e-c2354d06d4a1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Textarea(value='Hello, how are you?', description='Input:', layout=Layout(height='100px', width='100%'), place…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2cbb017722ee4bb8866b07f65b2b331e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(description='Translate', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "556003638c05459cb0734f61f58252f9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Textarea(value='', description='Output:', disabled=True, layout=Layout(height='100px', width='100%'), placehol…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b4b73f4e7ae34f59936680c9d377a352"
            }
          },
          "metadata": {}
        }
      ]
    }
  ]
}